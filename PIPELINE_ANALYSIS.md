# Academic CV Pipeline Analysis

## Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          STEP 1: SEED DATA                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ orcid_pull.py                                                            â”‚
â”‚   INPUT:  profiles.yaml â†’ ORCID ID                                      â”‚
â”‚   API:    https://pub.orcid.org/v3.0/{orcid}/works                     â”‚
â”‚   OUTPUT: data/processed/orcid_seed.json                                â”‚
â”‚   PURPOSE: Get initial list of works from ORCID profile                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STEP 2: OPENALEX ENRICHMENT                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ openalex_enrich.py                                                       â”‚
â”‚   INPUT:  ORCID ID from profiles.yaml                                   â”‚
â”‚   API:    https://api.openalex.org/authors/{orcid}                     â”‚
â”‚   OUTPUT: data/processed/openalex_author.json                           â”‚
â”‚           data/processed/openalex_works.json                             â”‚
â”‚   PURPOSE: Get comprehensive works list with DOIs, venues, types        â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜…â˜… (highest quality, normalized data)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STEP 3: CROSSREF ENRICHMENT                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ crossref_fill.py                                                         â”‚
â”‚   INPUT:  data/processed/openalex_works.json                            â”‚
â”‚   API:    https://api.crossref.org/works/{doi}                         â”‚
â”‚   OUTPUT: data/processed/crossref_by_doi.json                           â”‚
â”‚   PURPOSE: Get detailed metadata (funders, licenses, abstracts)         â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜…â˜… (authoritative DOI metadata)                           â”‚
â”‚   NOTES:  0.1s delay between requests (polite API usage)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 STEP 4: ALTERNATIVE SOURCE IMPORTS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ scholar_html_import.py                                                   â”‚
â”‚   INPUT:  data/raw/scholar_profile.html (saved webpage)                 â”‚
â”‚   OUTPUT: data/raw/scholar_html.json                                    â”‚
â”‚   EXTRACTS: 176 publications (title, authors, venue, year, citations)   â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜†â˜† (no DOIs, citation counts useful)                      â”‚
â”‚                                                                           â”‚
â”‚ researchgate_html_import.py                                              â”‚
â”‚   INPUT:  data/raw/researchgate_profile.html (saved webpage)            â”‚
â”‚   OUTPUT: data/raw/researchgate_html.json                               â”‚
â”‚   EXTRACTS: 65 publications (title, url, year)                          â”‚
â”‚   QUALITY: â˜…â˜…â˜†â˜†â˜† (limited metadata, many duplicates)                    â”‚
â”‚   FEATURES: Deduplication by title, URL cleaning                        â”‚
â”‚                                                                           â”‚
â”‚ researchgate_import.py                                                   â”‚
â”‚   INPUT:  data/raw/researchgate_export.csv (CSV export)                 â”‚
â”‚   OUTPUT: data/processed/researchgate_works.json                        â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜†â˜† (better than HTML, has DOIs sometimes)                 â”‚
â”‚                                                                           â”‚
â”‚ lattes_import.py                                                         â”‚
â”‚   INPUT:  data/raw/lattes.xml (Plataforma Lattes XML)                   â”‚
â”‚   OUTPUT: data/processed/lattes_works.json                              â”‚
â”‚   EXTRACTS: Publications only (articles, conference, books, chapters)   â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜…â˜† (authoritative for Brazilian context, has DOIs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                STEP 5: COMPREHENSIVE CV DATA EXTRACTION                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ lattes_comprehensive.py                                                  â”‚
â”‚   INPUT:  data/raw/lattes.xml                                           â”‚
â”‚   OUTPUT: data/processed/lattes_comprehensive.json                      â”‚
â”‚   EXTRACTS:                                                              â”‚
â”‚     â€¢ Personal info (name, ORCID, citation names)                       â”‚
â”‚     â€¢ Education (7 degrees with funding agencies)                       â”‚
â”‚     â€¢ Positions (37 professional positions)                             â”‚
â”‚     â€¢ Projects (21 research projects with funding)                      â”‚
â”‚     â€¢ Supervisions (15 PhD/Masters students)                            â”‚
â”‚     â€¢ Awards (8 honors)                                                  â”‚
â”‚     â€¢ Research areas (6 areas)                                          â”‚
â”‚     â€¢ Languages (2 languages)                                           â”‚
â”‚     â€¢ Teaching (7 courses)                                              â”‚
â”‚     â€¢ Committees (thesis defenses)                                      â”‚
â”‚   QUALITY: â˜…â˜…â˜…â˜…â˜… (complete academic career data)                        â”‚
â”‚   NOTES:  NOT merged with publications (separate use case)              â”‚
â”‚                                                                           â”‚
â”‚ cv_markdown_import.py                                                    â”‚
â”‚   INPUT:  data/raw/cv_markdown.md                                       â”‚
â”‚   OUTPUT: data/processed/cv_markdown.json                               â”‚
â”‚   EXTRACTS: 17 sections from existing CV                                â”‚
â”‚   PURPOSE: Baseline/reference for CV structure                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               STEP 6: PUBLICATION DEDUPLICATION & MERGE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ normalize_dedupe.py                                                      â”‚
â”‚   INPUTS (in priority order):                                           â”‚
â”‚     1. data/processed/openalex_works.json      (â˜…â˜…â˜…â˜…â˜… highest)          â”‚
â”‚     2. data/processed/lattes_works.json        (â˜…â˜…â˜…â˜…â˜†)                  â”‚
â”‚     3. scholar_bibtex (if available)           (â˜…â˜…â˜…â˜†â˜†)                  â”‚
â”‚     4. data/raw/scholar_html.json              (â˜…â˜…â˜…â˜†â˜†)                  â”‚
â”‚     5. data/processed/researchgate_works.json  (â˜…â˜…â˜…â˜†â˜†)                  â”‚
â”‚     6. data/raw/researchgate_html.json         (â˜…â˜…â˜†â˜†â˜† lowest)           â”‚
â”‚                                                                           â”‚
â”‚   ALGORITHM:                                                             â”‚
â”‚     Phase 1: DOI-based exact matching (outer join)                      â”‚
â”‚       - Merge records with same DOI                                     â”‚
â”‚       - Coalesce missing fields from lower priority sources             â”‚
â”‚                                                                           â”‚
â”‚     Phase 2: Fuzzy title matching for non-DOI records                   â”‚
â”‚       - Use rapidfuzz token_set_ratio scorer                            â”‚
â”‚       - Threshold: 92% similarity                                       â”‚
â”‚       - Fill in missing venue, URL, year from matches                   â”‚
â”‚                                                                           â”‚
â”‚     Phase 3: Add unique records from each source                        â”‚
â”‚       - Publications not matched by DOI or title                        â”‚
â”‚       - Preserves source-specific publications                          â”‚
â”‚                                                                           â”‚
â”‚   OUTPUT: data/processed/works_merged.json                              â”‚
â”‚   RESULT: Deduplicated publication list with best metadata              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STEP 7: CV RENDERING                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ render.py                                                                â”‚
â”‚   INPUTS:                                                                â”‚
â”‚     â€¢ data/processed/works_merged.json (publications)                   â”‚
â”‚     â€¢ data/processed/lattes_comprehensive.json (CV sections)            â”‚
â”‚     â€¢ templates/{awesome-cv,moderncv,markdown}.tex.j2                   â”‚
â”‚     â€¢ profiles.yaml (template choice)                                   â”‚
â”‚                                                                           â”‚
â”‚   PROCESS:                                                               â”‚
â”‚     1. Load merged publications                                         â”‚
â”‚     2. Load comprehensive CV data                                       â”‚
â”‚     3. Organize by type/year                                            â”‚
â”‚     4. Render chosen template with Jinja2                               â”‚
â”‚     5. Compile LaTeX to PDF (if needed)                                 â”‚
â”‚                                                                           â”‚
â”‚   OUTPUT: build/cv.{pdf,md,tex}                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Quality by Source

| Source | DOIs | Venues | Years | Authors | Citations | Abstracts | Funders |
|--------|------|--------|-------|---------|-----------|-----------|---------|
| OpenAlex | âœ“âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“ | âœ— | âœ— |
| Crossref | âœ“âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“ | âœ“âœ“ | âœ“âœ“âœ“ | âœ“âœ“âœ“ |
| Lattes | âœ“âœ“ | âœ“âœ“ | âœ“âœ“âœ“ | âœ— | âœ— | âœ— | âœ— |
| Scholar HTML | âœ— | âœ“ | âœ“âœ“ | âœ“ | âœ“âœ“âœ“ | âœ— | âœ— |
| RG HTML | âœ— | âœ— | âœ“ | âœ— | âœ— | âœ— | âœ— |
| RG CSV | âœ“ | âœ“âœ“ | âœ“âœ“ | âœ“ | âœ— | âœ— | âœ— |

## Current Issues & Recommendations

### âœ… Strengths:
1. **Smart priority system** â€” OpenAlex first, then authoritative sources
2. **Robust deduplication** â€” DOI exact match + 92% fuzzy title matching
3. **Multiple fallbacks** â€” HTML parsing when exports unavailable
4. **Comprehensive data** â€” Beyond publications (education, projects, etc.)
5. **Clean organization** â€” All raw files in `data/raw/`

### âš ï¸ Potential Issues:

1. **Crossref data not used in merge**
   - Crossref enrichment stored in `crossref_by_doi.json`
   - But `normalize_dedupe.py` doesn't read it
   - **Missing:** Abstracts, funders, licenses
   - **Fix:** Merge crossref data into OpenAlex records before deduplication

2. **ORCID seed data not used**
   - `orcid_pull.py` creates `orcid_seed.json`
   - But never consumed by pipeline
   - **Impact:** Minimal (OpenAlex gets same data via ORCID anyway)
   - **Fix:** Could remove or use as validation check

3. **Lattes comprehensive vs lattes_works duplication**
   - Two parsers reading same XML file
   - `lattes_import.py` â†’ publications only
   - `lattes_comprehensive.py` â†’ everything including publications
   - **Fix:** Could extract publications from comprehensive parse

4. **No integration of markdown CV**
   - `cv_markdown_import.py` parses existing CV
   - But data not used in rendering
   - **Purpose:** Unclear (baseline? validation?)

5. **Missing fields in merged output**
   - No author names in final merge (except from Scholar)
   - No citation counts (except from Scholar)
   - No abstracts (Crossref has them)
   - **Impact:** Less rich CV metadata

### ğŸ”§ Recommended Improvements:

**Priority 1: Integrate Crossref enrichment**
```python
# In normalize_dedupe.py, after loading OpenAlex:
crossref = json.load(open("data/processed/crossref_by_doi.json"))
for work in ox:
    doi = work.get("doi","").lower()
    if doi in crossref:
        # Add abstract, funders, license
        work["abstract"] = crossref[doi].get("abstract")
        work["funders"] = crossref[doi].get("funder", [])
        work["license"] = crossref[doi].get("license", [])
```

**Priority 2: Extract authors from OpenAlex**
```python
# OpenAlex has authorships with names
for w in ox:
    authors = [a.get("author",{}).get("display_name") 
               for a in w.get("authorships",[])]
    rows.append({
        # ... existing fields ...
        "authors": ", ".join(authors)
    })
```

**Priority 3: Add citation counts**
```python
# Merge Scholar citation counts into final output
# Scholar HTML has "citations" field
# Could add to merged records for metrics
```

**Priority 4: Validation reporting**
```python
# Add summary statistics at end of normalize_dedupe.py
print(f"\nSource contributions:")
print(f"  OpenAlex: {len(df_ox)} records")
print(f"  Lattes: {len(df_lt)} records")
print(f"  Scholar: {len(df_sch_html)} records")
print(f"  ResearchGate: {len(df_rg_html)} records")
print(f"  After dedup: {len(merged)} records")
print(f"  Unique DOIs: {merged['doi'].astype(bool).sum()}")
```

## Pipeline Execution Time Estimate

| Step | Time | Notes |
|------|------|-------|
| ORCID pull | ~1s | Single API call |
| OpenAlex | ~5-10s | Paginated (200 per page) |
| Crossref | ~5-30s | 0.1s Ã— number of DOIs |
| Scholar HTML | <1s | Local file parse |
| RG HTML | <1s | Local file parse |
| Lattes works | ~1s | XML parse |
| Lattes comprehensive | ~2s | Full XML parse |
| Markdown import | <1s | Text parse |
| Normalize/dedupe | ~2-5s | Fuzzy matching intensive |
| **TOTAL** | **~20-50s** | Depends on record count |

## Next Steps

Would you like me to:
1. **Fix Crossref integration** â€” Add abstracts/funders to merged output
2. **Add author extraction** â€” Get author names from OpenAlex
3. **Add validation stats** â€” Show deduplication metrics
4. **Optimize lattes parsing** â€” Single parser for publications + CV data
5. **Test full pipeline** â€” Run `make update` and check all outputs
